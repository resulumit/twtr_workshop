---
title: 'Data Analysis: Tweets'
output: html_document
---

```{r, setup, include=FALSE}

# setup for this document
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)

# install packages that does not need to be loaded
# install.packages("rmakdown")   # running this document   
# install.packages("textdata")   # sentiment dictionary

# load packages
library(tidytext)      # working with texts, sentiment analysis
library(wordcloud)     # creating wordclouds
library(doc2concrete)  # concreteness scores
library(dotwhisker)    # visualising regression results
library(tidyverse)     # most other operations

# load data
# change the strings if necessary, depending on the structure of your directory
mps <- read.csv("../data/mps.csv", na.strings = "")
twts <- read_rds("../data/tweets.rds")

```

## How to use this document

This R Markdown document includes the exercises for Part 6 of the workshop, [Working with Twitter Data in R](https://resulumit.com/teaching/twtr_workshop.html#1). It introduces each exercise with a subtitle, followed by some explanations and a code chunk. Type your code into these chunks to complete the exercises.

If these exercises are too easy for you, I recommend analysing your own dataset or conducting additional analyses on the same dataset in parallel. If they are on the other hand too hard,  you may wish to follow on with the `tweets_answers.Rmd` instead, which include the code. In that case, try to adopt the code to your data as well.

If you have the `rmarkdown` package installed, you can see the results in two ways:

1) by clicking on the *Knit* button on the RStudio menu
    - this runs the codes in all chunks   
    - the results will appear in a separate HTML document that will pop up
    
2) by clicking on the green arrow on the top-right corner of any chunk
    - this runs the code in that chunk only    
    - the result will appear immediately under the relevant chunk


### When were the tweets posted?

The dataset covers the second half of January 2021, from the 15^th^ to the 31^st^ of the month. However, it is likely that MPs were not equally active on Twitter on very day in this period. Visualise the number tweets sent on each date to find out. 

```{r, exercise_63, echo=FALSE}

# start with the `tweets` dataset 
twts %>%
  
# filter out retweets
  filter(is_retweet == FALSE) %>%

# turn the created_at variable into a date variable, with the `as.Date` function 
  mutate(date = as.Date(created_at)) %>%

# calculate how many tweets posted on each date
  count(date) %>%
  
# graph the result
  ggplot(aes(x = date, y = n)) +
  geom_line() +
  theme_bw() +
  scale_x_date(date_breaks = "2 day", date_labels = "%b %d") +
  ylab("Number of Tweets\n") + xlab("\nDate")

```

### What day of the week?

The previous graph shows regular dips. Do those dips occur on  specific days of the week, such as on the weekend? Visualise the average number tweets sent on each day of the week to find out. 

```{r, exercise_64, echo=FALSE}

# start with the `tweets` dataset 
twts %>%
  
# filter out retweets
  filter(is_retweet == FALSE) %>%

# add a day variable, for the day of tweets posted (e.g., Monday, Tuesday), and make sure R know the order
# and a week variable, becuase some days are in the dataset multiple times
  mutate(day = format(created_at, tz = "GB", "%A"),
         day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday",
                                      "Friday", "Saturday", "Sunday")),
         week = format(created_at, tz = "GB", "%W")) %>%
  
# calculate the average number of tweets per day of the week
  group_by(week, day) %>% 
  summarise(by_day = n()) %>%
  group_by(day) %>% 
  summarise(mean_tweets = mean(by_day)) %>%
  
# plot the dataset
  ggplot(aes(day, mean_tweets)) +
  geom_line(aes(group = 1), size = 1) +
  theme_bw() +
  ylab("Number of Tweets\n") + xlab("\nDays") +
  scale_x_discrete(breaks = c("Monday", "Tuesday", "Wednesday", "Thursday",
                              "Friday", "Saturday", "Sunday"))

```

### What time of the day?

Similarly, we could explore the time of the day that MPs tweet. Visualise the average number tweets sent on each hour of the day to explore. 

```{r, exercise_65, echo=FALSE}

# start with the `tweets` dataset 
twts %>%
  
# filter out retweets
  filter(is_retweet == FALSE) %>%
  
# add an hour variable, for the hour of tweets posted (e.g., 01 AM, 02 AM)
  mutate(hour = format(created_at, tz = "GB", "%H")) %>%
  
# calculate the average number of tweets posted at each hour
  group_by(hour) %>%
  summarise(n_tweets = n()) %>%
  
# plot the dataset
  ggplot(aes(hour, n_tweets)) +
  geom_line(aes(group = 1), size = 1) +
  theme_bw() +
  ylab("Number of Tweets\n") + xlab("\nHours")

```


### Which hastags were the most frequent?

Hastags categorise the content on Twitter. As such, they can be informative about what the tweets are about. What do MPs tweet about? To explore, visualise the top 20 most used hashtags in the dataset.

Recall that the hashtag variable includes lists of hastags if a tweet itself has more than one hashtag. This requires tokenising them first. 

```{r, exercise_66, echo=FALSE}

# start with the `tweets` dataset 
twts %>%

# filter out retweets, tweets with no hastags
  filter(is_retweet == FALSE & !is.na(hashtags)) %>%

# unlist the lists of hashtags to create strings
  group_by(status_id) %>%
  mutate(tidy_hashtags = str_c(unlist(hashtags), collapse = " ")) %>%

  ungroup() %>%
  
# unnest the hashtags
  unnest_tokens(output = unnested_hashtags, input = tidy_hashtags, token = "words") %>%

# count the number of times each hashtag is used
  count(unnested_hashtags) %>%

# limit the analysis to top 20 hashtags
  top_n(20) %>%
  
# plot the results
  ggplot(aes(y = reorder(unnested_hashtags, n),
             x = n)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  ylab("Top 20 Hashtags\n") + xlab("")

```


### Which words were the most frequent?

One could also look at the most frequently used words in tweets, again with a barplot. A popular alternative is a wordcloud. Create a wordcloud with top 50 most frequently used words. 

Note that you may or may not wish to clean the text first, with the operations covered in Part 4 of the workshop, before you proceed.

Use the `wordcloud` function from the package with the same name to visualise the data.

```{r, exercise_67, echo=FALSE}

# create a temporary dataframe, based on the 'tweets' dataset
twts_temp <- twts %>%

# unnest tweets into tweet-words
  unnest_tokens(output = token_tweets, input = text, token = "tweets") %>%

# remove stopwords
  anti_join(., stop_words, by = c("token_tweets" = "word")) %>%

# remove the text for the sign `&`
  filter(token_tweets != "amp") %>%

# count how many times each word is used
  count(token_tweets)

# plot the temporary dataframe with the `wordcloud` function
wordcloud(words = twts_temp$token_tweets,
          freq = twts_temp$n, max.words = 50)

```

### Sentiments by Hours of the Day

Do the tone of the tweets change with time of the day? To explore, visualise the average sentiments across different hours of the day. 

```{r, exercise_68, echo=FALSE}

df_sentiments <- get_sentiments("nrc") %>%
  mutate(sentiment_value = 1) %>%
  pivot_wider(names_from = sentiment, values_from = sentiment_value, values_fill = 0)

twts %>%
  filter(is_retweet == FALSE) %>% 
  unnest_tokens(output = token_tweets, input = text, token = "tweets") %>%
  left_join(., df_sentiments, by = c("token_tweets" = "word")) %>%
  mutate(hour = format(created_at, tz = "GB", "%H")) %>%
  group_by(hour) %>%
  summarise(positive_tone = (sum(positive, na.rm = TRUE) - sum(negative, na.rm = TRUE)) /
                            (sum(positive, na.rm = TRUE) + sum(negative, na.rm = TRUE))) %>%
  ungroup() %>%
  ggplot(aes(hour, positive_tone)) +
  geom_line(aes(group = 1), size = 1) +
  theme_bw() +
  ylab("Average Sentiment\n") + xlab("Hours")

```

### Sentiments across the time frame

Sentiments can also change depending on the day. To explore, visualise the average sentiments across the time frame under analysis. Optionally, conduct your analysis for each of the four main parties: Conservative, Labour, SNP, and LibDem.  


```{r, exercise_69, echo=FALSE}

# using the `get_sentiments` function, get the positive and negative sentiments from the nrc dictionary
df_sentiments <- get_sentiments("nrc") %>%
  
# pivot your data wider, so that every new word is a new row
  mutate(sentiment_value = 1) %>%
  pivot_wider(names_from = sentiment, values_from = sentiment_value, values_fill = 0)


# using the tweets dataset
twts %>%
  
# join the mps dataframe in
  left_join(., mps, by = "screen_name") %>% 
  
# filter out retweets, mps from smaller parties 
  filter(is_retweet == FALSE &
         party %in% c("Conservative", "Labour", "SNP", "LibDem")) %>% 
  
# unnest the remaining tweets into words
  unnest_tokens(output = token_tweets, input = text, token = "tweets") %>%
  
# join the sentiments dataframe in
  left_join(., df_sentiments, by = c("token_tweets" = "word")) %>%
  
# add a date variable, and group by date and party
  mutate(date = as.Date(created_at)) %>%
  group_by(party, date) %>%
  
# calculate the positivity score for these groups
  summarise(positive_tone = (sum(positive, na.rm = TRUE) - sum(negative, na.rm = TRUE)) /
                            (sum(positive, na.rm = TRUE) + sum(negative, na.rm = TRUE))) %>%
  ungroup() %>%
  
# visualise the results, faceting by party
  ggplot(aes(date, positive_tone)) +
  geom_line(aes(group = 1), size = 1) +
  theme_bw() +
  facet_wrap(. ~ party) +
  scale_x_date(date_breaks = "4 day", date_labels = "%b %d") +
  ylab("Average Sentiment\n") + xlab("")

```

### Sentiments in different types of tweets

Do the tone change with the types of tweets, such as those mention another user, reply, or quote them? To explore whether the sentiments differ between these types of tweets, conduct a simple correlation analysis.


```{r, exercise_70, echo=FALSE}

# using the `get_sentiments` function, get the positive and negative sentiments from the nrc dictionary
df_sentiments <- get_sentiments("nrc") %>%
  
# pivot your data wider, so that every new word is a new row
  mutate(sentiment_value = 1) %>%
  pivot_wider(names_from = sentiment, values_from = sentiment_value, values_fill = 0)

# calculate the positivity score for each tweet
df_tone <- twts %>%
  
  # unnest to tweet-words
  unnest_tokens(output = token_tweets, input = text, token = "tweets") %>%
  
# join the sentiments dataframe in
  left_join(., df_sentiments, by = c("token_tweets" = "word")) %>%
  
# calculate the positivity score for each tweet
  group_by(status_id) %>%
  summarise(positive_tone = (sum(positive, na.rm = TRUE) - sum(negative, na.rm = TRUE)) /
                            (sum(positive, na.rm = TRUE) + sum(negative, na.rm = TRUE))) %>%
  ungroup()
  

# create a temporary dataframe, from the `tweets` dataset
twts_temp <- twts %>%
  
# filter out retweets
  filter(is_retweet == FALSE) %>% 
  
# filter out self replies
  filter(!(screen_name == reply_to_screen_name) | is.na(reply_to_screen_name)) %>% 
  
# create a categorical variable for the remaining types of tweets
  mutate(mentions = case_when(is.na(mentions_screen_name) ~ 0,
                              !is.na(reply_to_screen_name) & mentions_screen_name == reply_to_screen_name ~ 0,
                              !is.na(mentions_screen_name) ~ 1),
         quotes = if_else(is_quote == TRUE, 1, 0),
         replies = if_else(!is.na(reply_to_screen_name), 1, 0)) %>% 
  
# join in the positivit scores
  left_join(., df_tone, by = "status_id")
  

# run a regression model, with positivity score as the dependent variable and tweet types (tweets, replies, quotes) as independent variables
m <- lm(positive_tone ~ mentions + quotes + replies, data = twts_temp)

# plot the regression results with the `dwplot` function
dwplot(m, dot_args = list(size = 3.2), whisker_args = list(size = 1.5)) +
  theme_bw() +
  theme(legend.position = "none") +
  geom_vline(xintercept = 0, linetype = 2) +
  xlab("Regression Coefficient") +
  scale_y_discrete(labels = function(x) stringr::str_to_title(x))

```

### Concreteness in different types of tweets

Conduct the same comparision, this time for the concreteness of tweets.

```{r, exercise_71, echo=FALSE}

# calculate the positivity score for each tweet
df_concreteness <- twts %>%
  
# unnest to tweet-words
  unnest_tokens(output = token_tweets, input = text, token = "tweets") %>%
  
# join the concreteness dataframe in
  left_join(., mturk_list, by = c("token_tweets" = "Word")) %>%
  
  group_by(status_id) %>%
  
  summarise(concrete_tone = if_else(all(is.na(Conc.M)), NA_real_, sum(Conc.M, na.rm = TRUE))) %>%
  ungroup()


# create a temporary dataframe, from the `tweets` dataset
twts_temp <- twts %>%
  
# filter out retweets
  filter(is_retweet == FALSE) %>% 
  
# filter out self replies
  filter(!(screen_name == reply_to_screen_name) | is.na(reply_to_screen_name)) %>% 
  
# create a categorical variable for the remaining types of tweets
  mutate(mentions = case_when(is.na(mentions_screen_name) ~ 0,
                              !is.na(reply_to_screen_name) & mentions_screen_name == reply_to_screen_name ~ 0,
                              !is.na(mentions_screen_name) ~ 1),
         quotes = if_else(is_quote == TRUE, 1, 0),
         replies = if_else(!is.na(reply_to_screen_name), 1, 0)) %>% 
  
# join in the positivit scores
  left_join(., df_concreteness, by = "status_id")
  

# run a regression model, with positivity score as the dependent variable and tweet types (tweets, replies, quotes) as independent variables
m <- lm(concrete_tone ~ Mentions + Quotes + Replies, data = twts_temp)

# plot the regression results with the `dwplot` function
dwplot(m, dot_args = list(size = 3.2), whisker_args = list(size = 1.5)) +
  theme_bw() +
  theme(legend.position = "none") +
  geom_vline(xintercept = 0, linetype = 2) +
  xlab("Regression Coefficient")+
  scale_y_discrete(labels = function(x) stringr::str_to_title(x))


```

### Concreteness by Hours of the Day

Could it be that how concrete we are changes with the passing of the day? To explore, run a simple regression model, with the average concreteness of tweets as the dependent variable, and hours as the independent variable. 

```{r, exercise_72, echo=FALSE}

twts_temp <- twts %>%
  filter(is_retweet == FALSE) %>% 
  mutate(hour = format(created_at, tz = "GB", "%H")) %>%
  unnest_tokens(output = token_tweets, input = text, token = "tweets") %>%
  anti_join(stop_words, by = c("token_tweets" = "word")) %>%
  left_join(., mturk_list, by = c("token_tweets" = "Word")) %>%
  group_by(status_id, hour) %>%
  summarise(concrete_tone = if_else(all(is.na(Conc.M)), NA_real_, sum(Conc.M, na.rm = TRUE)))

# run a regression model, with concreteness score as the dependent variable and hours as independent variable
m <- lm(concrete_tone ~ hour, data = twts_temp)

# plot the regression results with the `dwplot` function
dwplot(m, dot_args = list(size = 3.2), whisker_args = list(size = 1.5)) +
  theme_bw() +
  theme(legend.position = "none") +
  geom_vline(xintercept = 0, linetype = 2) +
  xlab("Regression Coefficient")

```


### Something else interesting about MPs

Conduct another tweet-based analysis of the `tweets` dataset --- something that interests you. Work in groups of two, on the same datasets of one that you might have. Present your results to the class.


```{r, exercise_73, echo=FALSE}


```


### Something interesting from your own data

Conduct another tweet-based analysis of the dataset that you have collected. Work on your own. Present your results to the class.

```{r, exercise_74, echo=FALSE}



```
