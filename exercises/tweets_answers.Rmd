---
title: 'Data Analysis: Tweets'
output: html_document
---

```{r, setup, include=FALSE}

# setup for this document
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)

# install packages that does not need to be loaded
# install.packages("rmakdown")   # running this document   
# install.packages("textdata")   # sentiment dictionary

# load packages
library(tidytext)      # working with texts, sentiment analysis
library(wordcloud)     # creating wordclouds
library(doc2concrete)  # concreteness scores
library(dotwhisker)    # visualising regression results
library(tidyverse)     # most other operations

# load data
mps <- read.csv("../data/mps.csv", na.strings = "")
twts <- read_rds("../data/tweets.rds")

```

## How to use this document

This R Markdown document includes the exercises for Part 6 of the workshop, [Working with Twitter Data in R](https://resulumit.com/teaching/twtr_workshop.html#1). It introduces each exercise with a subtitle, followed by some explanations and a code chunk. Type your code into these chunks to complete the exercises.

If these exercises are too easy for you, I recommend analysing your own dataset or conducting additional analyses on the same dataset in parallel. If they are on the other hand too hard,  you may wish to follow on with the `tweets_answers.Rmd` instead, which include the code. In that case, try to adopt the code to your data as well.

If you have the `rmarkdown` package installed, you can see the results in two ways:

1) by clicking on the *Knit* button on the RStudio menu
    - this runs the codes in all chunks   
    - the results will appear in a separate HTML document that will pop up
    
2) by clicking on the green arrow on the top-right corner of any chunk
    - this runs the code in that chunk only    
    - the result will appear immediately under the relevant chunk


### When were the tweets posted?

The dataset covers the second half of January 2021, from the 15^th^ to the 31^st^ of the month. However, it is likely that MPs were not equally active on Twitter on very day in this period. Visualise the number tweets sent on each date to find out. 

```{r, exercise_63, echo=FALSE}

# start with the `tweets` dataset 
twts %>%

# turn the the created_at variable into a date variable, with the as.Date function 
  mutate(date = as.Date(created_at)) %>%

# calculate how many tweets posted on each date
  count(date) %>%
  
# graph the result
  ggplot(aes(x = date, y = n)) +
  geom_line() +
  theme_bw() +
  scale_x_date(date_breaks = "2 day", date_labels = "%b %d") +
  ylab("Number of Tweets\n") + xlab("\nDate")

```

### What day of the week?

The previous graph shows regular dips. Do those dips occur on  specific days of the week, such as on the weekend? Visualise the average number tweets sent on each day of the week to find out. 

```{r, exercise_64, echo=FALSE}

twts %>%
  filter(is_retweet == FALSE) %>%
  mutate(day = format(created_at, tz = "GB", "%A"),
         day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday",
                                      "Friday", "Saturday", "Sunday")),
         week = format(created_at, tz = "GB", "%W")) %>%
  group_by(week, day) %>% 
  summarise(by_day = n()) %>%
  group_by(day) %>% 
  summarise(mean_tweets = mean(by_day)) %>%
  ggplot(aes(day, mean_tweets)) +
  geom_line(aes(group = 1), size = 1) +
  theme_bw() +
  ylab("Number of Tweets\n") + xlab("\nDays") +
  scale_x_discrete(breaks = c("Monday", "Tuesday", "Wednesday", "Thursday",
                              "Friday", "Saturday", "Sunday"))

```

### What time of the day?

Similarly, we could explore the time of the day that MPs tweet. Visualise the average number tweets sent on each hour of the day to explore. 

```{r, exercise_65, echo=FALSE}

twts %>%
  mutate(hour = format(created_at, tz = "GB", "%H")) %>%
  group_by(hour) %>%
  summarise(n_tweets = n()) %>%
  ggplot(aes(hour, n_tweets)) +
  geom_line(aes(group = 1), size = 1) +
  theme_bw() +
  ylab("Number of Tweets\n") + xlab("\nHours")

```


### Which hastags were the most frequent?

Hastags categorise the content on Twitter. As such, they can be informative about what the tweets are about. What do MPs tweet about? To explore, visualise the top 20 most used hashtags in the dataset.

Recall that the hashtag variable includes lists of hastags if a tweet itself has more than one hashtag. This requires tokenising them first. 

```{r, exercise_66, echo=FALSE}

twts %>%

  filter(is_retweet == FALSE & !is.na(hashtags)) %>%

# unlist the lists of hashtags to create strings
  group_by(status_id) %>%
  mutate(tidy_hashtags = str_c(unlist(hashtags), collapse = " ")) %>%

  ungroup() %>%
  
# split the string, create a new variable called `da_tweets`
  unnest_tokens(output = unnested_hashtags, input = tidy_hashtags, token = "words") %>%

  count(unnested_hashtags) %>%
  top_n(20) %>%
  ggplot(aes(y = reorder(unnested_hashtags, n),
             x = n)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  ylab("Top 20 Hashtags\n") + xlab("")

```


### Which words were the most frequent?

One could also look at the most frequently used words in tweets, again with a barplot. A popular alternative is a wordcloud. Create a wordcloud with top 50 most frequently used words. 

Note that you may or may not wish to clean the text first, with the operations covered in Part 4 of the workshop, before you proceed.

Use the `wordcloud` function from the package with the same name to visualise the data.

```{r, exercise_67, echo=FALSE}

twts_temp <- twts %>%
  unnest_tokens(output = token_tweets, input = text, token = "tweets") %>%
  anti_join(., stop_words, by = c("token_tweets" = "word")) %>%
  filter(token_tweets != "amp") %>%
  count(token_tweets)

wordcloud(words = twts_temp$token_tweets,
          freq = twts_temp$n, max.words = 50)

```

### Sentiments by Hours of the Day

Do the tone of the tweets change with time of the day? To explore, visualise the average sentiments across different hours of the day. 

```{r, exercise_68, echo=FALSE}

df_sentiments <- get_sentiments("nrc") %>%
  mutate(sentiment_value = 1) %>%
  pivot_wider(names_from = sentiment, values_from = sentiment_value, values_fill = 0)

twts %>%
  filter(is_retweet == FALSE) %>% 
  unnest_tokens(output = token_tweets, input = text, token = "tweets") %>%
  left_join(., df_sentiments, by = c("token_tweets" = "word")) %>%
  mutate(hour = format(created_at, tz = "GB", "%H")) %>%
  group_by(hour) %>%
  summarise(positive_tone = (sum(positive, na.rm = TRUE) - sum(negative, na.rm = TRUE)) /
                            (sum(positive, na.rm = TRUE) + sum(negative, na.rm = TRUE))) %>%
  ungroup() %>%
  ggplot(aes(hour, positive_tone)) +
  geom_line(aes(group = 1), size = 1) +
  theme_bw() +
  ylab("Average Sentiment\n") + xlab("Hours")

```

### Sentiments across the time frame

Sentiments can also change depending on the day. To explore, visualise the average sentiments across the time frame under analysis. Optionally, conduct your analysis for each of the four main parties: Conservative, Labour, SNP,- and LibDem.  


```{r, exercise_69, echo=FALSE}

df_sentiments <- get_sentiments("nrc") %>%
  mutate(sentiment_value = 1) %>%
  pivot_wider(names_from = sentiment, values_from = sentiment_value, values_fill = 0)

twts %>%
  left_join(., mps, by = "screen_name") %>% 
  filter(is_retweet == FALSE &
         party %in% c("Conservative", "Labour", "SNP", "LibDem")) %>% 
  unnest_tokens(output = token_tweets, input = text, token = "tweets") %>%
  left_join(., df_sentiments, by = c("token_tweets" = "word")) %>%
  mutate(date = as.Date(created_at)) %>%
  group_by(party, date) %>%
  summarise(positive_tone = (sum(positive, na.rm = TRUE) - sum(negative, na.rm = TRUE)) /
                            (sum(positive, na.rm = TRUE) + sum(negative, na.rm = TRUE))) %>%
  ungroup() %>%
  ggplot(aes(date, positive_tone)) +
  geom_line(aes(group = 1), size = 1) +
  theme_bw() +
  facet_wrap(. ~ party) +
  scale_x_date(date_breaks = "4 day", date_labels = "%b %d") +
  ylab("Average Sentiment\n") + xlab("")

```

### Sentiments in different types of tweets

Do the tone change from regular tweets to replies and quotes? Recall our discussion that there is a like button but there is no button to dislike, which could bias the sentiments in replies towards negative. There are also the quotes tweets, which some users prefer to direct replies. To explore whether the sentiments differ between these types of tweets, conduct a simple correlation analysis.

```{r, exercise_70, echo=FALSE}

df_sentiments <- get_sentiments("nrc") %>%
  mutate(sentiment_value = 1) %>%
  pivot_wider(names_from = sentiment, values_from = sentiment_value, values_fill = 0)

twts_temp <- twts %>%
  filter(is_retweet == FALSE) %>% 
  mutate(replies = ifelse(is.na(reply_to_status_id), 1, 0),
         quotes = ifelse(is_quote == TRUE, 1, 0)) %>%
  unnest_tokens(output = token_tweets, input = text, token = "tweets") %>%
  anti_join(stop_words, by = c("token_tweets" = "word")) %>%
  left_join(., df_sentiments, by = c("token_tweets" = "word")) %>%
  group_by(status_id, replies, quotes) %>%
  summarise(positive_tone = (sum(positive, na.rm = TRUE) - sum(negative, na.rm = TRUE)) /
              (sum(positive, na.rm = TRUE) + sum(negative, na.rm = TRUE)))

m <- lm(positive_tone ~ replies + quotes, data = twts_temp)

# plot the regression results with the `dwplot` function
dwplot(m, dot_args = list(size = 3.2), whisker_args = list(size = 1.5)) +
  theme_bw() +
  theme(legend.position = "none") +
  geom_vline(xintercept = 0, linetype = 2) +
  xlab("Regression Coefficient")

```

### Concreteness in different types of tweets

Conduct the same compassion, this time for the concreteness of tweets.

```{r, exercise_71, echo=FALSE}

twts_temp <- twts %>%
  filter(is_retweet == FALSE) %>% 
  mutate(replies = ifelse(is.na(reply_to_status_id), 1, 0),
         quotes = ifelse(is_quote == TRUE, 1, 0)) %>%
  unnest_tokens(output = token_tweets, input = text, token = "tweets") %>%
  anti_join(stop_words, by = c("token_tweets" = "word")) %>%
  left_join(., mturk_list, by = c("token_tweets" = "Word")) %>%
  group_by(status_id, replies, quotes) %>%
  summarise(concrete_tone = if_else(all(is.na(Conc.M)), NA_real_, sum(Conc.M, na.rm = TRUE)))

m <- lm(concrete_tone ~ replies + quotes, data = twts_temp)

# plot the regression results with the `dwplot` function
dwplot(m, dot_args = list(size = 3.2), whisker_args = list(size = 1.5)) +
  theme_bw() +
  theme(legend.position = "none") +
  geom_vline(xintercept = 0, linetype = 2) +
  xlab("Regression Coefficient")

```

### Concreteness by Hours of the Day

Could it be that how concrete we are changes with the passing of the day? To explore, run a simple regression model, with the average concreteness of tweets as the dependent variable, and hours as the independent variable. 

```{r, exercise_72, echo=FALSE}

twts_temp <- twts %>%
  filter(is_retweet == FALSE) %>% 
  mutate(hour = format(created_at, tz = "GB", "%H")) %>%
  unnest_tokens(output = token_tweets, input = text, token = "tweets") %>%
  anti_join(stop_words, by = c("token_tweets" = "word")) %>%
  left_join(., mturk_list, by = c("token_tweets" = "Word")) %>%
  group_by(status_id, hour) %>%
  summarise(concrete_tone = if_else(all(is.na(Conc.M)), NA_real_, sum(Conc.M, na.rm = TRUE)))

m <- lm(concrete_tone ~ hour, data = twts_temp)

# plot the regression results with the `dwplot` function
dwplot(m, dot_args = list(size = 3.2), whisker_args = list(size = 1.5)) +
  theme_bw() +
  theme(legend.position = "none") +
  geom_vline(xintercept = 0, linetype = 2) +
  xlab("Regression Coefficient")

```


### Something else interesting about MPs

Conduct another tweet-based analysis of the `tweets` dataset --- something that interests you. Work in groups of two, on the same datasets of one that you might have. Present your results to the class.


```{r, exercise_73, echo=FALSE}


```


### Something interesting from your own data

Conduct another tweet-based analysis of the dataset that you have collected. Work on your own. Present your results to the class.

```{r, exercise_74, echo=FALSE}



```
